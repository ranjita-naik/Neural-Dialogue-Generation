{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note : TensorFlow >= 1.10 and enable eager execution\n",
    "\n",
    "# Based on https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/eager/python/\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WordIndex():\n",
    "    \"\"\"\n",
    "    This class implements \"word to id\" and \"id to word\" mapping functions. \n",
    "    It's used in the embedding layer of the Seq2Seq model. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # stores word to id mappings        \n",
    "        self.word2idx = defaultdict(lambda: len(self.word2idx))\n",
    "        self.pad = self.word2idx[\"<pad>\"]\n",
    "        self.unk = self.word2idx[\"<unk>\"]\n",
    "\n",
    "        \n",
    "    def create_forward_index(self, sentences):\n",
    "        word_indices = []\n",
    "        for sent in sentences:\n",
    "            words = [self.word2idx[word] for word in sent] \n",
    "            word_indices.append(words)\n",
    "        return word_indices\n",
    "    \n",
    "    def create_reverse_index(self):\n",
    "        # stores id to word mappings\n",
    "        # this will freeze the dictionary. If the word doesn't exist in the dictionary, <unk> will be returned.\n",
    "        self.word2idx =  defaultdict(lambda: self.unk, self.word2idx)\n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    This class implements the functionality for reading and pre-processing the dataset.   \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.word_index = WordIndex()\n",
    "        self.max_length_source = -1\n",
    "        self.max_length_target = -1\n",
    "        self.source_tensor = []\n",
    "        self.target_tensor = []\n",
    "        self.buffer_size = -1\n",
    "        self.batch_size = -1\n",
    "        self.n_batch = -1\n",
    "        \n",
    "        # load the dataset from path location\n",
    "        self.load_dataset()\n",
    "    \n",
    "    \n",
    "    def unicode_to_ascii(self, s):\n",
    "        # Converts the unicode file to ascii\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn')\n",
    "    \n",
    "    def max_length(self, tensor):\n",
    "        return max(len(t) for t in tensor)\n",
    "\n",
    "\n",
    "    def preprocess_sentence(self, w):\n",
    "        w = self.unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "        # creating a space between a word and the punctuation following it\n",
    "        # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "        w = w.rstrip().strip()\n",
    "    \n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> ' + w + ' <end>'\n",
    "        return w\n",
    "    \n",
    "    \n",
    "    def load_dataset(self):\n",
    "        # load the data set and split it into <question, answer> pairs\n",
    "        lines = open(self.path, encoding='UTF-8').read().strip().lower().split('\\n')\n",
    "        word_pairs = [[self.preprocess_sentence(w).split(' ') for w in l.split('\\t')]  for l in lines]\n",
    "        \n",
    "        source =  [ source for source, target in word_pairs]\n",
    "        target =  [ target for source, target in word_pairs]     \n",
    "        \n",
    "        # maps the words to ids\n",
    "        source_tensor = self.word_index.create_forward_index(source)\n",
    "        target_tensor = self.word_index.create_forward_index(target)\n",
    "        \n",
    "        self.word_index.create_reverse_index()\n",
    "        \n",
    "        # Calculate max_length of input and output tensor\n",
    "        # Here, we'll set those to the longest sentence in the dataset\n",
    "        self.max_length_source, self.max_length_target = self.max_length(source_tensor), self.max_length(target_tensor)\n",
    "    \n",
    "        # Padding the input and output tensor to the maximum length\n",
    "        self.source_tensor = tf.keras.preprocessing.sequence.pad_sequences(source_tensor, \n",
    "                                                                 maxlen=self.max_length_source,\n",
    "                                                                 padding='post')\n",
    "    \n",
    "        self.target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                  maxlen=self.max_length_target, \n",
    "                                                                  padding='post')\n",
    "        \n",
    "        self.buffer_size = len(self.source_tensor)\n",
    "        self.batch_size = 64\n",
    "        self.n_batch = self.buffer_size//self.batch_size\n",
    "        \n",
    "        self.tf_dataset = tf.data.Dataset.from_tensor_slices((self.source_tensor, self.target_tensor)).shuffle(self.buffer_size)\n",
    "        self.tf_dataset = self.tf_dataset.batch(self.batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "    # the code automatically does that.\n",
    "\n",
    "    if tf.test.is_gpu_available():\n",
    "        return tf.keras.layers.CuDNNGRU(units, \n",
    "                                    return_sequences=True, \n",
    "                                    return_state=True, \n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class  Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "                \n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state\n",
    "    \n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation.\n",
    "    \"\"\"\n",
    "    embedding_dim = 128\n",
    "    units = 512\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    max_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel():\n",
    "    \n",
    "    def __init__(self, config, path):\n",
    "        self.config = config\n",
    "        self.dataset = Dataset(path)\n",
    "        self.encoder = Encoder(len(self.dataset.word_index.word2idx), self.config.embedding_dim, \n",
    "                               self.config.units, self.dataset.batch_size)\n",
    "        self.decoder = Decoder(len(self.dataset.word_index.word2idx), self.config.embedding_dim, \n",
    "                               self.config.units, self.dataset.batch_size)        \n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in range(self.config.max_epochs):\n",
    "            start = time.time()\n",
    "    \n",
    "            hidden = self.encoder.initialize_hidden_state()\n",
    "            total_loss = 0\n",
    "    \n",
    "            for (batch, (inp, targ)) in enumerate(self.dataset.tf_dataset):\n",
    "                loss = 0\n",
    "        \n",
    "                with tf.GradientTape() as tape:\n",
    "                    enc_output, enc_hidden = self.encoder(inp, hidden)\n",
    "                    dec_hidden = enc_hidden\n",
    "                    dec_input = tf.expand_dims([self.dataset.word_index.word2idx['<start>']] * self.dataset.batch_size, 1)       \n",
    "            \n",
    "                    # Teacher forcing - feeding the target as the next input\n",
    "                    for t in range(1, targ.shape[1]):\n",
    "                        # passing enc_output to the decoder\n",
    "                        predictions, dec_hidden = self.decoder(dec_input, dec_hidden)\n",
    "                \n",
    "                        loss += self.loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                        # using teacher forcing\n",
    "                        dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "                batch_loss = (loss / int(targ.shape[1]))\n",
    "                total_loss += batch_loss\n",
    "                variables = self.encoder.variables + self.decoder.variables\n",
    "                gradients = tape.gradient(loss, variables)\n",
    "                self.config.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "                if batch % 100 == 0:\n",
    "                    print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    \n",
    "        print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / self.dataset.n_batch))\n",
    "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    \n",
    "    def evaluate(self,sentence):\n",
    "        sentence = self.dataset.preprocess_sentence(sentence)\n",
    "        inputs = [self.dataset.word_index.word2idx[i] for i in sentence.split(' ')]\n",
    "        inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=self.dataset.max_length_source, padding='post')\n",
    "        inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "        result = ''\n",
    "        hidden = [tf.zeros((1, self.config.units))]\n",
    "        enc_out, enc_hidden = self.encoder(inputs, hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([self.dataset.word_index.word2idx['<start>']], 0)\n",
    "\n",
    "        for t in range(self.dataset.max_length_target):\n",
    "            predictions, dec_hidden = self.decoder(dec_input, dec_hidden)\n",
    "                \n",
    "            # Un-comment the following for non-greedy decoding \n",
    "            #predicted_id = tf.multinomial(predictions, num_samples=1)[0][0].numpy()\n",
    "            predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "            \n",
    "            result += self.dataset.word_index.idx2word[predicted_id] + ' '\n",
    "\n",
    "            if self.dataset.word_index.idx2word[predicted_id] == '<end>':\n",
    "                return result, sentence\n",
    "        \n",
    "            # the predicted ID is fed back into the model\n",
    "            dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        return result, sentence\n",
    "    \n",
    "    \n",
    "    def loss_function(self,real, pred):\n",
    "        mask = 1 - np.equal(real, 0)\n",
    "        loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "        return tf.reduce_mean(loss_)\n",
    "    \n",
    "    def converse(self, sentence):\n",
    "        result, sentence = self.evaluate(sentence)\n",
    "        \n",
    "        print('Input : {}'.format(sentence))\n",
    "        print('Response : {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_dialogue_agent():\n",
    "    dialogue_corpus = \"./data/test.txt\"\n",
    "    \n",
    "    config = Config()\n",
    "    model = Seq2SeqModel(config, dialogue_corpus)\n",
    "    model.train()\n",
    "    model.converse(\"how was your day?\")\n",
    "    model.converse(\"what's your favourite band?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_dialogue_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
